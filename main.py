from fastapi import FastAPI
from pydantic import BaseModel
from starlette.responses import FileResponse
from fastapi.middleware.cors import CORSMiddleware
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import load_model
from music21 import stream, note, chord, instrument,  percussion, converter
from create_generator_model import get_notes, LATENT_DIMENSION, get_off_set,get_notes_from_file
import math
from pathlib import Path
from random import randrange
import os
import time
origins = [
    "http://localhost.tiangolo.com",
    "https://localhost.tiangolo.com",
    "http://localhost",
    "http://localhost:8080",
]
instr = instrument.Violin()
generator_model = load_model("generator_model-20M-1024.h5")
off_set = get_off_set()
notes = get_notes_from_file()
unpich_SnareDrum = note.Note("D2",instrument=instrument.SnareDrum)
unpich_HiHatCymbal = note.Note("F#2",instrument=instrument.HiHatCymbal)
unpich_Percussion = note.Note("A#0",instrument=instrument.Percussion)
unpich_CrashCymbals = note.Note("D#3",instrument=instrument.CrashCymbals)
unpich_TomTom = note.Note("E2",instrument=instrument.TomTom)
unpich_BassDrum = note.Note("C2",instrument=instrument.BassDrum)

def create_melody(prediction_output,instrumental,off_set,key_index,note_interval = -0):
    offset = 0
    output_notes = []
    for item in prediction_output:
        pattern = item
        if pattern.startswith("Q"):
           offset += float(pattern.replace("Q",""))
        else:
        # pattern is a chord
            offset += max(float(off_set[key_index]) + note_interval,0)
            if pattern == 'R':
                    output_notes.append(note.Rest())
            elif pattern.startswith("D."):
                note_in_set = pattern.split(".")
                notes = []
                for instrument_name in note_in_set:
                  if instrument_name == 'Snare Drum':
                    notes.append(unpich_SnareDrum)
                  elif instrument_name == 'Hi-Hat Cymbal':
                    notes.append(unpich_HiHatCymbal)
                  elif instrument_name == 'Percussion':
                    notes.append(unpich_Percussion)
                  elif instrument_name == 'Crash Cymbals':
                    notes.append(unpich_CrashCymbals)
                  elif instrument_name == 'Tom-Tom':
                    notes.append(unpich_TomTom)
                  elif instrument_name == 'Bass Drum':
                    notes.append(unpich_BassDrum)
                  elif instrument_name != 'D':
                    print(instrument_name)
                new_chord = percussion.PercussionChord(notes)
                new_chord.offset = offset
                output_notes.append(new_chord)
            elif ('.' in pattern) or pattern.isdigit():
                notes_in_chord = pattern.split('.')
                notes = []
                for current_note in notes_in_chord:
                    new_note = note.Note(int(current_note))
                    new_note.storedInstrument = instrumental
                    notes.append(new_note)
                new_chord = chord.Chord(notes)
                new_chord.offset = offset
                output_notes.append(new_chord)
                # offset += 0.5
        # pattern is a note
            else:
                new_note = note.Note(pattern)
                new_note.offset = offset
                new_note.storedInstrument = instrumental
                output_notes.append(new_note)
            key_index += 1
    midi_stream = stream.Part(output_notes)
    midi_stream.insert(instrumental)
    return midi_stream
def generate_music(generator_model, latent_dim, n_vocab, notes, length=500):
    """ Generate new music using the trained generator model """
    # Create random noise as input to the generator
    noise = np.random.normal(0, 1, (1, latent_dim))
    predictions = generator_model.predict(noise)
    
    # Scale back the predictions to the original range
    pred_notes = [x * (n_vocab / 2) + (n_vocab / 2) for x in predictions[0]]
    
    # Map generated integer indices to note names
    pitchnames = sorted(set(item for item in notes))
    int_to_note = dict((number, note) for number, note in enumerate(pitchnames))
    pred_notes_mapped = [int_to_note[int(x)] for x in pred_notes]
    
    return pred_notes_mapped[:length]

def create_multi_midi(params,filename):
        # Load the trained generator model
    try:
    # Load the processed notes and get the number of unique pitches
        n_vocab = len(set(notes))
        # Generate new music sequence
        piano_music = generate_music(generator_model, LATENT_DIMENSION, n_vocab,notes)
        music_range = len(piano_music)
        # create note and chord objects based on the values generated by the model
        i = randrange(len(off_set)-music_range)
        midi_stream = stream.Score()
        speed  = 0
        if params.offset == 1:
            speed = -0.2
        if params.offset == 3:
            speed = 0.5
        if params.instrument == 'piano':
            music = generate_music(generator_model, LATENT_DIMENSION, n_vocab,notes)
            midi_stream.insert(create_melody(music,instrument.Piano(),off_set,i,speed))
        if params.instrument == 'flute':
            music = generate_music(generator_model, LATENT_DIMENSION, n_vocab,notes)
            midi_stream.insert(create_melody(music,instrument.Flute(),off_set,i,speed))
        if params.instrument == 'guitar':
            music = generate_music(generator_model, LATENT_DIMENSION, n_vocab,notes)
            midi_stream.insert(create_melody(music,instrument.AcousticGuitar(),off_set,i,speed))
        if params.instrument == 'violin':
            music = generate_music(generator_model, LATENT_DIMENSION, n_vocab,notes)
            midi_stream.insert(create_melody(music,instrument.Violin(),off_set,i,speed))
        if params.instrument == 'ukulele':
            music = generate_music(generator_model, LATENT_DIMENSION, n_vocab,notes)
            midi_stream.insert(create_melody(music,instrument.Ukulele(),off_set,i,speed))
        if params.drum:
            music = generate_music(generator_model, LATENT_DIMENSION, n_vocab,notes)
            midi_stream.insert(create_melody(music,instrument.BassDrum(),off_set,i,speed))
        midi_stream.write('midi', fp='midi/{}.mid'.format(filename))
        return True
    except:
        return False
def download(file_path):

    if os.path.isfile(file_path):
        return FileResponse(file_path)
    return None

app = FastAPI()
app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

class GenMusic(BaseModel):
    instrument: str
    drum: bool
    offset: int

@app.post("/Gen-Music")
def read_root(item:GenMusic):
    filename = str(time.time()).replace(".","-")
    result = create_multi_midi(item,filename)
    if result:
        # os.system(f"fluidsynth -ni {'midi/{}.mid'.format(filename)} {'midi/{}.mid'.format(filename)} -F {'midi/{}.mp3'.format(filename)} -r 44100")
        return {"mp3File":download('midi/{}.mp3'.format(filename)),
                "midFile":download('midi/{}.mid'.format(filename))}
    else:
        return {"fail":True}
